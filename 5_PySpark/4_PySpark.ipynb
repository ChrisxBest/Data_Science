{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c28c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"HDD_logs/\"\n",
    "\n",
    "import os\n",
    "file_list = sorted(os.listdir(data_dir))\n",
    "print(\"Number of Files:\" ,len(file_list))\n",
    "\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f919011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Spark\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"Python Spark SQL HDD Analysis\")\n",
    "        .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(data_dir+file_list[0], header=True)\n",
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(data_dir+file_list[1], header=True)\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab367d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_2 = df1.union(df2)\n",
    "print(df1_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d21e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_list[0])\n",
    "df = spark.read.csv(data_dir + file_list[0], header = True)\n",
    "\n",
    "for file in file_list[1:]:\n",
    "    print(file)\n",
    "    tmp_df = spark.read.csv(data_dir + file , header = True)\n",
    "    df = df.union(tmp_df)\n",
    "    \n",
    "print(\"Number of Rows : \", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
